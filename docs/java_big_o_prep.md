# Big `O`

[назад](../README.md)

* [Что такое `Big O Notation` и для чего она используется](#что-такое-big-o-notation-и-для-чего-она-используется)
* [Временная сложность доступа к элементу в массиве и почему](#временная-сложность-доступа-к-элементу-в-массиве-и-почему)
* [Какова сложность поиска в массиве](#какова-сложность-поиска-в-массиве)
* [Какая временная сложность у операций вставки и удаления в связном списке](#какая-временная-сложность-у-операций-вставки-и-удаления-в-связном-списке)
* [В чем разница между `O(n)` и `O(log n)`](#в-чем-разница-между-on-и-olog-n)
* [Какова сложность поиска в хеш-таблице и почему](#какова-сложность-поиска-в-хеш-таблице-и-почему)
* [Какова временная сложность сортировки пузырьком, вставками и выбором](#какова-временная-сложность-сортировки-пузырьком-вставками-и-выбором)
* [Какова временная сложность `бинарного поиска`](#какова-временная-сложность-бинарного-поиска)
* [Какова сложность алгоритма `сортировки слиянием` и почему](#какова-сложность-алгоритма-сортировки-слиянием-и-почему)
* [Какова сложность `быстрой сортировки` в среднем и худшем случаях](#какова-сложность-быстрой-сортировки-в-среднем-и-худшем-случаях)
* [Что такое `худший`, `средний` и `лучший` случаи в контексте `Big O`](#что-такое-худший-средний-и-лучший-случаи-в-контексте-big-o)
* [Чем отличается `O(n²)` от `O(2^n)`](#чем-отличается-on²-от-o2^n)
* [Какова временная сложность обхода дерева (например, в глубину или ширину)](#какова-временная-сложность-обхода-дерева-например-в-глубину-или-ширину)
* [Какова сложность добавления элемента в `ArrayList` и в `LinkedList` в `Java](#какова-сложность-добавления-элемента-в-arraylist-и-в-linkedlist-в-java)
* [Какова сложность удаления элемента из середины `ArrayList` и `LinkedList`](#какова-сложность-удаления-элемента-из-середины-arraylist-и-linkedlist)
* [Что такое амортизированная временная сложность](#что-такое-амортизированная-временная-сложность)
* [Какова сложность доступа, вставки и удаления в стеке и очереди](#какова-сложность-доступа-вставки-и-удаления-в-стеке-и-очереди)
* [Что лучше по производительности: массив или связный список, и в каких случаях](#что-лучше-по-производительности:-массив-или-связный-список-и-в-каких-случаях)
* [Какова сложность алгоритма сортировки кучей (`heapsort`)](#какова-сложность-алгоритма-сортировки-кучей-heapsort)
* [Какова сложность алгоритма поиска в графе (например, `DFS` и `BFS`)](#какова-сложность-алгоритма-поиска-в-графе-например-dfs-и-bfs)
* [Какова сложность создания бинарного дерева поиска](#какова-сложность-создания-бинарного-дерева-поиска)
* [Как `асимптотическая сложность` влияет на производительность с увеличением размера входных данных](#как-асимптотическая-сложность-влияет-на-производительность-с-увеличением-размера-входных-данных)
* [Как изменится временная сложность, если использовать рекурсию вместо итерации](#как-изменится-временная-сложность-если-использовать-рекурсию-вместо-итерации)
* [Какова сложность операций вставки и удаления в бинарной куче](#какова-сложность-операций-вставки-и-удаления-в-бинарной-куче)
* [Как определить временную сложность рекурсивных функций](#как-определить-временную-сложность-рекурсивных-функций)
* [Какова сложность алгоритма Дейкстры для поиска кратчайшего пути](#какова-сложность-алгоритма-дейкстры-для-поиска-кратчайшего-пути)
* [Влияет ли удвоение объема входных данных на время выполнения `O(log n)` алгоритма](#влияет-ли-удвоение-объема-входных-данных-на-время-выполнения-olog-n-алгоритма)
* [Какова сложность получения размера хеш-таблицы](#какова-сложность-получения-размера-хеш-таблицы)
* [Какова временная сложность разбиения массива (partitioning) в быстрой сортировке](#какова-временная-сложность-разбиения-массива-partitioning-в-быстрой-сортировке)
* [Как вычислить пространственную сложность алгоритма](#как-вычислить-пространственную-сложность-алгоритма)


## Что такое `Big O Notation` и для чего она используется

`Big O Notation` — это математическая нотация, используемая для описания верхней границы сложности алгоритма с точки зрения времени выполнения или занимаемого пространства в зависимости от размера входных данных. Она позволяет анализировать эффективность алгоритма и делать предсказания о его производительности при различных объемах данных.

### Назначение Big O Notation:
1. **Оценка производительности**:  
Big O Notation помогает оценить, насколько быстро увеличивается время выполнения алгоритма или затраты памяти с увеличением размера входных данных. Это критически важно для понимания того, насколько хорошо алгоритм будет масштабироваться.

2. **Сравнение алгоритмов**:    
Позволяет сравнивать различные алгоритмы и выбирать более эффективные решения для конкретных задач и условий.

3. **Абстракция от деталей реализации**:    
Big O фокусируется на общих тенденциях роста времени выполнения или затрат памяти, а не на конкретных значениях, что делает ее универсальной для анализа алгоритмов независимо от языка программирования или специфики системы.

###  Примеры Big O Notation:
* **`O(1)` - Константная сложность**:    
Время выполнения не зависит от размера входных данных. Например, `доступ к элементу массива по индексу`.

* **O(log n) - Логарифмическая сложность**:    
Время выполнения увеличивается логарифмически по отношению к размеру входных данных. Пример: `бинарный поиск`.

* **O(n) - Линейная сложность**:    
Время выполнения увеличивается линейно с увеличением размера входных данных. Например, `простой поиск в массиве`.

* **O(n log n) - Линейно-логарифмическая сложность**:     
Часто встречается в эффективных алгоритмах сортировки, таких как `быстрая сортировка`.

* **O(n²) - Квадратичная сложность**:    
Время выполнения увеличивается квадратично с увеличением размера входных данных. Пример: `пузырьковая сортировка`.

Важно понимать, что `Big O Notation` **описывает наихудшее поведение алгоритма в теоретических пределах** и не всегда отражает его производительность в реальных условиях.

[наверх](#big-o)


## Временная сложность доступа к элементу в массиве и почему

Временная сложность доступа к элементу в массиве обычно является O(1), что означает константную сложность. Это потому, что массивы в большинстве языков программирования, включая Java, представляют собой структуры данных, основанные на непрерывном блоке памяти.

### Почему это O(1):

* **Прямой доступ**: В массиве каждый элемент имеет уникальный индекс, и вы можете получить доступ к любому элементу напрямую, используя его индекс. Независимо от размера массива, доступ к элементу по индексу занимает одинаковое количество времени.

* **Расчет адреса**: Когда вы обращаетесь к элементу массива, компьютер вычисляет его адрес в памяти на основе базового адреса массива, размера элементов и индекса. Этот расчет производится за константное время.

### Пример:

Предположим, у вас есть массив `int[] array = {10, 20, 30, 40, 50};` и вы хотите получить доступ к элементу с индексом 3 (`array[3]`). Независимо от того, насколько велик массив, операция доступа к этому элементу займет одинаковое количество времени, потому что вы напрямую обращаетесь к конкретному месту в памяти.

### Важные моменты:

* **Тип массива**: Описанная выше характеристика относится к простым массивам (например, статическим массивам в Java). Однако, если вы используете структуры данных, основанные на массивах, но с дополнительной логикой (например, динамические массивы, такие как ArrayList в Java), другие операции, такие как вставка или удаление, могут иметь разную сложность.

* **Размер массива**: Хотя доступ к элементу не зависит от размера массива и всегда является O(1), размер физической памяти, выделенной для массива, может повлиять на производительность из-за кэширования и других низкоуровневых факторов.

* **Отличие от других структур данных**: В отличие от массивов, доступ к элементам в других структурах данных, таких как связные списки, может иметь большую временную сложность (например, O(n) для связного списка), поскольку потребуется последовательный обход элементов.

[наверх](#big-o)

## Какова сложность поиска в массиве

Сложность поиска в массиве зависит от того, является ли массив отсортированным или неотсортированным, а также от используемого метода поиска.

### Для неотсортированного массива:

#### Линейный поиск:
* В неотсортированном массиве поиск обычно выполняется с помощью линейного поиска.

* Сложность линейного поиска — O(n), где n — количество элементов в массиве.

* Это связано с тем, что в худшем случае может потребоваться проверить каждый элемент массива для нахождения целевого значения.

### Для отсортированного массива:

#### Бинарный поиск:

* В отсортированном массиве поиск обычно выполняется с помощью бинарного поиска.

* Сложность бинарного поиска — O(log n).

* Бинарный поиск эффективен, так как он делит массив на половины с каждым шагом, быстро сужая область поиска.

### Дополнительные соображения:

* **Выбор метода поиска**: Выбор между линейным и бинарным поиском зависит от того, отсортирован ли массив. Сортировка массива перед выполнением бинарного поиска может быть нецелесообразной, если массив используется для однократного поиска, поскольку сортировка потребует дополнительного времени.
* **Влияние на производительность**: Несмотря на то что бинарный поиск значительно быстрее в больших массивах, для маленьких массивов разница в производительности может быть несущественной, и использование линейного поиска может быть более простым решением.

В целом, понимание сложности различных методов поиска в массивах помогает оптимизировать алгоритмы и улучшить производительность программ.

[наверх](#big-o)

## Какая временная сложность у операций вставки и удаления в связном списке

Временная сложность операций вставки и удаления в связном списке зависит от места, где эти операции выполняются:

### Вставка в связный список:
1. **В начале списка (голова)**:   
Вставка элемента в начало связного списка (голову) выполняется за O(1), так как требуется только изменить указатель первого элемента.
2. **В конце списка (хвост) в односвязном списке**:   
Если у вас нет ссылки на последний элемент, вставка в конец списка потребует обхода всего списка, что делает эту операцию **O(n)**, где **n** — количество элементов в списке.

3. **В конце списка (хвост) в двусвязном списке или если есть ссылка на последний элемент**:   
Вставка в конец списка выполняется за **O(1)**, если вы поддерживаете ссылку на последний элемент или используете двусвязный список.

4. **Вставка после определенного узла**:    
Если вы вставляете элемент после известного узла, это также выполняется за **O(1)**.

### Удаление из связного списка:

1. **Удаление из начала списка**:   
Удаление первого элемента списка выполняется за **O(1)**, так как требуется только изменить указатель на голову списка.

2. **Удаление из конца или середины списка в односвязном списке**:   
Удаление из середины или конца списка обычно требует предварительного обхода до предшествующего элемента, что делает операцию **O(n)**.

3. **Удаление из двусвязного списка**:   
В двусвязном списке удаление узла, до которого вы можете напрямую обратиться, выполняется за **O(1)**, так как не требуется обходить список для поиска предыдущих элементов.

### Дополнительные соображения:   

1. **Производительность**:    
Хотя связные списки обеспечивают быструю вставку и удаление, они могут быть менее производительными по сравнению с массивами при частых доступах к элементам из-за необходимости обхода.

2. **Потребление памяти**:    
Связные списки требуют дополнительной памяти для хранения указателей на следующие (и предыдущие в случае двусвязных списков) элементы.

Временная сложность операций вставки и удаления в связном списке делает его подходящим для определенных сценариев использования, где часто выполняются вставки и удаления, особенно когда точная позиция этих операций заранее известна.

[наверх](#big-o)

## В чем разница между `O(n)` и `O(log n)`

**Big O Notation** описывает сложность алгоритма с точки зрения времени выполнения или пространственных требований в зависимости от размера входных данных **(n)**. Различие между **O(n)** и **O(log n)** заключается в том, как время выполнения алгоритма увеличивается с увеличением размера входных данных.

### O(n) - Линейная Сложность
* **Описание**: Алгоритм с линейной сложностью O(n) означает, что время выполнения алгоритма увеличивается линейно и пропорционально размеру входных данных.
* **Пример**: Простой цикл по массиву или списку, где вы проверяете или изменяете каждый элемент, является примером линейной сложности.
* **Визуализация**: Если вы построите график, где горизонтальная ось представляет размер входных данных, а вертикальная ось — время выполнения, то линия будет прямой, идущей вверх.

### O(log n) - Логарифмическая Сложность
* **Описание**: Алгоритм с логарифмической сложностью O(log n) означает, что время выполнения увеличивается логарифмически по отношению к размеру входных данных. Это означает, что с каждым удвоением размера входных данных время выполнения увеличивается не вдвое, а на фиксированную величину.
* **Пример**: Бинарный поиск в отсортированном массиве является классическим примером логарифмической сложности.
* **Визуализация**: На графике это представлено кривой, которая быстро поднимается на небольших размерах входных данных, но замедляется с увеличением размера.

### Ключевые Различия
* **Скорость Роста**: В O(n) время растет прямо пропорционально размеру входных данных, в то время как в O(log n) оно растет гораздо медленнее.
* **Эффективность**: O(log n) обычно более эффективен, чем O(n), особенно на больших наборах данных.
* **Примеры Алгоритмов**: Линейный поиск характерен для O(n), в то время как деление массива на половины в бинарном поиске является характерным для O(log n).

Понимание разницы между **O(n)** и **O(log n)** помогает выбирать наиболее подходящие алгоритмы и структуры данных для конкретных задач, оптимизируя производительность и эффективность программ.

[наверх](#big-o)
 
## Какова сложность поиска в хеш-таблице и почему

Сложность поиска в хеш-таблице, *в идеальном случае*, составляет **O(1)**, то есть константная. Это означает, что **время, необходимое для поиска элемента, не зависит от общего количества элементов в хеш-таблице**. Однако на практике сложность может увеличиваться из-за коллизий.

### Почему идеальная сложность составляет O(1):
1. **Механизм Хеширования**:   
Хеш-таблицы используют хеш-функцию для преобразования ключей в индексы хеш-таблицы. Эта функция вычисляется очень быстро.

2. **Прямой Доступ**:   
Вычисленный индекс используется для прямого доступа к соответствующему слоту таблицы, где хранится значение. Это делает процесс поиска очень быстрым.

### Влияние Коллизий:
1. **Коллизии Хеш-Функций**:  
Коллизии происходят, когда два разных ключа дают один и тот же хеш. В этом случае элементы хранятся в одной и той же позиции таблицы, обычно в виде списка (цепочки).

2. **Увеличение Сложности Поиска**:   
Когда происходит коллизия, поиск элемента может требовать обхода всех элементов в цепочке. В худшем случае, если все ключи попадают в один слот, сложность поиска может увеличиться до **O(n)**, где **n** — количество элементов в хеш-таблице.

### Факторы, Влияющие на Эффективность:
1. **Качество Хеш-Функции**:   
Хорошая хеш-функция равномерно распределяет ключи по слотам хеш-таблицы, минимизируя количество коллизий.

2. **Фактор Заполнения**:    
Фактор заполнения таблицы (отношение количества элементов к количеству слотов) влияет на вероятность коллизий. Чем выше фактор заполнения, тем больше вероятность коллизий.

3. **Стратегии Разрешения Коллизий**:    
Методы разрешения коллизий, такие как цепочки или открытая адресация, также влияют на производительность.

В целом, хеш-таблицы обеспечивают очень быстрый доступ к данным в среднем случае, но важно учитывать коллизии и правильно выбирать хеш-функцию и стратегию разрешения коллизий для оптимизации производительности.

[наверх](#big-o)

## Какова временная сложность сортировки пузырьком, вставками и выбором

### 1. Сортировка пузырьком (Bubble Sort)
* Худший случай: O(n²)
* Средний случай: O(n²)
* Лучший случай: O(n) (когда массив уже отсортирован)

**Объяснение**: Алгоритм проходит по массиву несколько раз, сравнивая соседние элементы и меняя их местами, если они не отсортированы. Каждый проход гарантирует, что на своё место встанет один элемент. В худшем случае (если массив изначально отсортирован в обратном порядке) потребуется `n²` сравнений.

### 2. Сортировка вставками (Insertion Sort)
* Худший случай: O(n²)
* Средний случай: O(n²)
* Лучший случай: O(n) (когда массив уже отсортирован)

**Объяснение**: Алгоритм проходит по массиву, и для каждого элемента находит правильное место среди уже отсортированных элементов. В худшем случае для каждого нового элемента потребуется перемещать его через весь отсортированный массив, что требует `O(n²)` операций.

### 3. Сортировка выбором (Selection Sort)
* Худший случай: O(n²)
* Средний случай: O(n²)
* Лучший случай: O(n²)

**Объяснение**: Алгоритм на каждой итерации находит минимальный элемент в неотсортированной части массива и меняет его с первым элементом этой части. Этот процесс всегда требует `n²` сравнений вне зависимости от того, как отсортирован массив.

**Итог**:

* Все три алгоритма имеют временную сложность `O(n²)` в худшем и среднем случае, но **сортировка вставками в лучшем случае работает за `O(n)`, если массив уже отсортирован**.

* Сортировка пузырьком также может работать быстрее в лучшем случае — за `O(n)`, если добавить оптимизацию, которая завершает алгоритм, если массив уже отсортирован.

[наверх](#big-o)

## Какова временная сложность `бинарного поиска`

Временная сложность бинарного поиска:
* Худший случай: O(log n)
* Средний случай: O(log n)
* Лучший случай: O(1) (если центральный элемент является искомым)

**Объяснение**:
* Бинарный поиск работает на отсортированном массиве и каждый раз делит массив пополам, выбирая половину, в которой может находиться искомый элемент. Это уменьшает область поиска в два раза на каждой итерации.

* Следовательно, для массива длиной n бинарный поиск выполняет около `log₂(n)` сравнений, что приводит к логарифмической временной сложности `O(log n)`.

[наверх](#big-o)

## Какова сложность алгоритма `сортировки слиянием` и почему

### Временная сложность сортировки слиянием (Merge Sort):

* Худший случай: O(n log n)
* Средний случай: O(n log n)
* Лучший случай: O(n log n)

### Почему временная сложность O(n log n)?

1. Разделение массива:

    * Алгоритм рекурсивно делит массив на две части до тех пор, пока каждая часть не станет длиной `1`. Этот процесс разделения происходит за `log n` уровней рекурсии, так как на каждом уровне массив делится пополам.

2. Слияние массивов:

    * На каждом уровне рекурсии происходит слияние двух отсортированных половин. Для слияния двух частей потребуется выполнить работу, пропорциональную числу элементов — `O(n)`.

3. Общая сложность:

    * Поскольку на каждом уровне рекурсии требуется `O(n)` операций для слияния, а всего уровней рекурсии `log n`, общая временная сложность сортировки слиянием составляет `O(n log n)`.

**Итог**:

Сортировка слиянием всегда имеет временную сложность `O(n log n)`, независимо от того, как элементы распределены в массиве.

[наверх](#big-o)

## Какова сложность `быстрой сортировки` в среднем и худшем случаях

### Временная сложность быстрой сортировки (Quick Sort):
* Средний случай: `O(n log n)`
* Худший случай: `O(n²)`

**Объяснение**:
1. Средний случай — `O(n log n)`:

    * В среднем быстрая сортировка делит массив на две примерно равные части, а затем рекурсивно сортирует каждую часть.

    * Деление происходит на каждом уровне рекурсии, что составляет `log n` уровней.

    * На каждом уровне требуется пройти по всем элементам массива, что даёт сложность `O(n)`.

    * В итоге, общая сложность в среднем составляет `O(n log n)`, что делает быструю сортировку одной из самых эффективных для большинства случаев.

2. Худший случай — `O(n²)`:

    * Худший случай возникает, если каждый раз при делении массива на части одна из частей оказывается пустой или почти пустой. Это может произойти, если в качестве опорного элемента (`pivot`) выбирается минимальный или максимальный элемент в массиве, например, если массив уже отсортирован или все элементы одинаковые.
    * В этом случае на каждом уровне деления уменьшается только один элемент, что приводит к n уровням рекурсии, и на каждом уровне выполняется `O(n)` операций.
    * Таким образом, сложность в худшем случае будет `O(n²)`.


**Итог**:

* Средняя сложность: `O(n log n)` — быстрая сортировка работает эффективно для большинства массивов.

* Худшая сложность: `O(n²)` — происходит при неудачном выборе опорного элемента, что приводит к несбалансированным разделениям массива.

[наверх](#big-o)

## Что такое `худший`, `средний` и `лучший` случаи в контексте `Big O`

В контексте анализа алгоритмов с использованием нотации `Big O`, термины худший, средний и лучший случаи описывают различные сценарии выполнения алгоритма в зависимости от входных данных. Эти случаи помогают оценить производительность алгоритма при разных условиях.

### 1. Худший случай (Worst-case):
* Худший случай показывает, как долго будет работать алгоритм в наименее благоприятных для него условиях. Это сценарий, при котором алгоритм выполнит максимальное количество операций.

* `Big O` в худшем случае описывает верхнюю границу времени выполнения алгоритма.

* Например, для сортировки вставками худший случай возникает, если массив изначально отсортирован в обратном порядке, что приведет к необходимости большого числа перестановок. В этом случае сложность — `O(n²)`.

### 2. Средний случай (Average-case):
* Средний случай описывает время выполнения алгоритма для типичных входных данных, которое является усреднённым результатом по всем возможным наборам данных.

* `Big O` в среднем случае оценивает производительность алгоритма на большинстве входных данных.

* Для быстрой сортировки средний случай возникает, если массив разделяется на части относительно равномерно. В таком случае сложность — `O(n log n)`.

### 3. Лучший случай (Best-case):
* Лучший случай показывает наименьшее количество операций, которое выполнит алгоритм в самых благоприятных условиях.

* `Big `O в лучшем случае указывает нижнюю границу времени выполнения алгоритма.

* Например, для сортировки вставками лучший случай возникает, если массив уже отсортирован. Тогда сложность будет `O(n)`, так как алгоритм просто пройдет по массиву без перестановок.

**Пример**:

Для алгоритма `сортировки вставками`:

* Худший случай: O(n²) — массив отсортирован в обратном порядке.
* Средний случай: O(n²) — элементы массива случайно перемешаны.
* Лучший случай: O(n) — массив уже отсортирован.

**Заключение**:

* Худший случай важен для оценки максимального времени выполнения, чтобы алгоритм не оказался слишком медленным в критических ситуациях.

* Средний случай описывает ожидаемую производительность алгоритма на реальных данных.

* Лучший случай показывает, как быстро алгоритм может завершиться в идеальных условиях, но чаще всего этот случай не является ориентиром для оптимизации.

[наверх](#big-o)

## Чем отличается `O(n²)` от `O(2^n)`

`O(n²)` и `O(2^n)` представляют разные уровни роста времени выполнения алгоритма в зависимости от размера входных данных, и они существенно отличаются по скорости увеличения.

### 1. O(n²) (квадратичная сложность):
* Характер роста: Квадратичная сложность означает, что время выполнения увеличивается пропорционально квадрату размера входных данных. Если размер входных данных увеличивается, время выполнения растет относительно медленно по сравнению с экспоненциальной сложностью.

* Пример: Если n = 10, количество операций будет примерно 100 (10²). Если n = 100, количество операций будет 10 000 (100²).

* Типичные алгоритмы: Алгоритмы с двумя вложенными циклами, такие как сортировка вставками и пузырьковая сортировка в худшем случае.

### 2. O(2^n) (экспоненциальная сложность):
* Характер роста: Экспоненциальная сложность означает, что время выполнения удваивается с каждым добавлением нового элемента в размер входных данных. Это приводит к крайне быстрому росту времени выполнения при увеличении размера данных.

* Пример: Если n = 10, количество операций будет примерно 1024 (2¹⁰). Если n = 20, количество операций будет уже 1 048 576 (2²⁰).

* Типичные алгоритмы: Обычно встречается в рекурсивных алгоритмах, таких как решение задачи с перебором всех комбинаций, например, задача о полном переборе, подмножества или задача о Ханойских башнях.

### Различие в темпе роста:
* O(n²) увеличивается относительно медленно по сравнению с O(2^n). Например:
    * Для n = 10: O(n²) ≈ 100 операций, O(2^n) ≈ 1024 операций.
    * Для n = 20: O(n²) ≈ 400 операций, O(2^n) ≈ 1 048 576 операций.

* O(n²) подходит для алгоритмов, работающих с умеренными объемами данных, тогда как O(2^n) становится непрактичным даже при небольшом увеличении входных данных.

### Итог:
* O(n²): Квадратичный рост, медленнее, чем экспоненциальный, и приемлем для небольших и средних наборов данных.

* O(2^n): Экспоненциальный рост, крайне неэффективен для больших входных данных, так как время выполнения резко возрастает даже при небольшом увеличении размера данных.

[наверх](#big-o)

## Какова временная сложность обхода дерева (например, в глубину или ширину)

Временная сложность обхода дерева (как в глубину, так и в ширину) составляет `O(n)`, где `n` — количество узлов в дереве.

### Объяснение:
1. Обход в глубину (`DFS` — Depth-First Search):

    * Алгоритм рекурсивно или с использованием стека посещает каждый узел дерева.
    * В процессе обхода каждый узел посещается ровно один раз, и каждое ребро (связь между узлами) также обрабатывается один раз.
    * Следовательно, общее количество операций пропорционально количеству узлов в дереве.

2. Обход в ширину (`BFS` — Breadth-First Search):

* Алгоритм использует очередь для посещения каждого уровня дерева поочередно.

* Как и в случае с `DFS`, каждый узел посещается ровно один раз, а каждое ребро проверяется один раз.

* Время выполнения также зависит от числа узлов и составляет `O(n)`.

**Заключение**:

Для любого типа обхода дерева (в глубину или в ширину), каждый узел посещается один раз, и поэтому временная сложность обхода дерева — `O(n)`, где `n` — общее количество узлов в дереве.

[наверх](#big-o)

## Какова сложность добавления элемента в `ArrayList` и в `LinkedList` в `Java`

Сложность добавления элемента в `ArrayList` и `LinkedList` в `Java`:

### 1. ArrayList:

* Добавление в конец:
    * В среднем случае: `O(1)` — Если в массиве есть место для нового элемента, операция добавления выполняется за постоянное время.
    * В худшем случае: `O(n)` — Когда массив становится полным, его размер удваивается, и все элементы копируются в новый массив. Эта операция занимает `O(n)`, где n — текущий размер списка.

* Добавление в начало или середину:
    * `O(n)` — Требуется сдвигать все элементы, которые следуют за местом вставки.

### 2. LinkedList:

* Добавление в конец:
    * `O(1)` — В случае двусвязного списка элемент добавляется за постоянное время, так как LinkedList содержит ссылку на последний элемент.
* Добавление в начало:
    * `O(1)` — Добавление в начало также занимает постоянное время, так как не требуется сдвиг элементов.
* Добавление в середину:
    * `O(n)` — Требуется пройти по списку до нужной позиции для вставки, так как доступ к элементам осуществляется последовательно.

**Заключение**:
* `ArrayList`: Добавление в конец в среднем случае выполняется за `O(1)`, но при увеличении размера массива сложность может возрасти до `O(n)`. Добавление в начало или середину — `O(n)`.

* `LinkedList`: Добавление в начало или конец всегда выполняется за `O(1)`, тогда как добавление в середину требует `O(n)` времени для поиска позиции.

[наверх](#big-o)

## Какова сложность удаления элемента из середины `ArrayList` и `LinkedList`

Сложность удаления элемента из середины в `ArrayList` и `LinkedList`:

### 1. ArrayList:

* Удаление из середины:
    * `O(n)` — После удаления элемента требуется сдвигать все элементы, которые идут после удаленного элемента, чтобы заполнить образовавшуюся "дыру".

* Пример: Если удалить элемент на позиции `i`, все элементы с индекса `i+1` до конца списка нужно сдвинуть на одну позицию влево.

### 2. LinkedList:

* Удаление из середины:
    * `O(n)` — В `LinkedList` необходимо последовательно пройти по элементам до нужной позиции (сложность поиска элемента — `O(n)`). Однако после нахождения элемента его удаление занимает `O(1)`, так как меняются только ссылки узлов.

* Пример: Для удаления элемента на позиции i нужно пройти через все предыдущие узлы до этой позиции, а затем обновить ссылки соседних узлов.

**Заключение**:
* `ArrayList`: Удаление элемента из середины требует `O(n)` из-за необходимости сдвига элементов.

* `LinkedList`: Удаление элемента из середины также требует `O(n)`, но здесь основное время тратится на поиск нужного элемента, так как доступ к элементам последовательный.

[наверх](#big-o)

## Что такое амортизированная временная сложность

**Амортизированная временная сложность** — это техника анализа алгоритмов, которая позволяет оценить среднюю производительность операций в серии действий, даже если отдельные операции могут быть дорогими (занимать много времени). В отличие от традиционного анализа, который фокусируется на худшем случае для каждой операции, амортизированная сложность оценивает среднюю стоимость одной операции при выполнении ряда операций, сглаживая редкие "дорогие" операции за счёт множества "дешёвых".

### Как это работает:
* В некоторых алгоритмах отдельные операции могут выполняться очень быстро (например, за `O(1)`), но время выполнения одной операции может иногда резко возрасти (например, до `O(n)`). Амортизированный анализ показывает, что в среднем при многократном выполнении таких операций общая сложность будет ниже, чем кажется при рассмотрении только худшего случая.

### **Пример**: Амортизированная сложность динамического массива (`ArrayList` в `Java`)
Рассмотрим динамический массив (например, `ArrayList` в `Java`):

* Операция добавления (`append`) в конец списка обычно выполняется за `O(1)`, так как добавление элемента требует лишь помещения значения в следующую свободную ячейку.

* Однако когда массив заполняется, его нужно увеличить, создавая новый массив большего размера и копируя в него все элементы из старого массива. Это может потребовать `O(n)` операций, если массив содержал n элементов.

На первый взгляд, кажется, что время добавления может быть `O(n)`, когда требуется увеличение массива. Но эта операция происходит не на каждое добавление, а лишь иногда, когда массив заполняется. Большинство добавлений всё же выполняется за `O(1)`.

### Амортизированный анализ:
* Пусть начальный размер массива равен 1, и каждый раз, когда он заполняется, его размер удваивается.

* На первой вставке не требуется копирование — это `O(1)`.

* На втором вставке тоже `O(1)`.

* На третьей вставке размер массива удваивается, и требуется копирование 2 элементов — это `O(n)`.

* Но если смотреть на серию вставок (например, на 4, 8, 16 вставок и так далее), видно, что каждая удорожающая операция компенсируется множеством дешевых операций `O(1)`.

Таким образом, хотя в худшем случае операция вставки может занять `O(n)`, амортизированная сложность серии операций вставки остается `O(1)` в среднем на одну операцию.

**Заключение**:
* Амортизированная временная сложность помогает более точно оценить алгоритмы, в которых одна операция может быть дорогой, но встречается редко, и компенсируется множеством дешевых операций.

* Эта техника анализа полезна для понимания алгоритмов, таких как динамические массивы, хэш-таблицы (где нужно увеличивать хэш-таблицу), и некоторых структур данных (например, куч).

[наверх](#big-o)

## Какова сложность доступа, вставки и удаления в стеке и очереди

Временная сложность операций в стеке и очереди:
### 1. Стек (`Stack`):
* Доступ (`Access`): `O(n)`

    * Стек работает по принципу `LIFO` (последний вошел — первый вышел), поэтому прямого доступа к элементам, кроме верхнего (последнего), нет. Чтобы получить элемент в середине или начале стека, необходимо пройти через все предыдущие элементы.

* Вставка (`Push`): `O(1)`

    * Добавление нового элемента в стек происходит на вершине, и это делается за постоянное время, так как не требуется перемещения других элементов.

* Удаление (`Pop`): `O(1)`

    * Удаление верхнего элемента также происходит за постоянное время, так как операция затрагивает только последний элемент.
### 2. Очередь (`Queue`):
* Доступ (`Access`): `O(n)`

    * В очереди, работающей по принципу `FIFO` (первый вошел — первый вышел), прямого доступа к элементам, кроме первого, нет. Чтобы получить элемент, находящийся в середине или конце, нужно пройти через все предыдущие элементы.

* Вставка (`Enqueue`): `O(1)`

    * Добавление нового элемента в конец очереди выполняется за постоянное время, так как не требуется перемещения других элементов.

* Удаление (`Dequeue`): `O(1)`

    * Удаление первого элемента в очереди также выполняется за постоянное время, так как эта операция затрагивает только первый элемент.

Таким образом, вставка и удаление в стеке и очереди происходят за `O(1)`, а доступ к произвольным элементам требует `O(n)` времени.

[наверх](#big-o)

## Что лучше по производительности: массив или связный список, и в каких случаях

`Array` и `Linked List` — это две основные структуры данных, каждая из которых имеет свои сильные и слабые стороны в разных сценариях использования.

### Массивы:
* Доступ к элементам: `O(1)` (быстрый доступ)
    * Массивы предоставляют быстрый доступ к элементам по индексу, так как они хранятся в смежных блоках памяти.
    * Пример: `arr[5]` возвращает элемент за постоянное время `O(1)`.

* Вставка/Удаление:
    * Вставка/Удаление в конец: `O(1)` (если есть свободное место).

    * Вставка/Удаление в середину или начало: `O(n)` (так как требуется сдвиг всех последующих элементов).

* Размер:
    * Массивы имеют фиксированный размер (если используются простые массивы), что может потребовать перераспределения памяти (как в случае с динамическими массивами, такими как `ArrayList`), что потребует `O(n)` времени при расширении.

### Связные списки:
* Доступ к элементам: `O(n)` (медленный доступ)

    * Связный список предоставляет только последовательный доступ к элементам. Чтобы получить доступ к элементу, нужно пройти через предыдущие узлы.

* Вставка/Удаление:

    * Вставка/Удаление в начало: `O(1)` (если доступен указатель на голову списка).

    * Вставка/Удаление в середину или конец: `O(n)` (требуется пройти до нужного узла).

* Размер:

    * Связные списки динамически изменяются в размере. Это удобно, когда вы не знаете заранее, сколько элементов потребуется хранить, так как не нужно выделять память заранее.

### Когда лучше использовать массив:
1. Когда требуется быстрый доступ по индексу: Если вашей программе нужен частый доступ к элементам по индексу, массив обеспечивает `O(1)` доступ, что намного быстрее, чем в связном списке.
2. Когда количество элементов известно заранее: Если вы заранее знаете количество элементов и не планируете часто изменять размер структуры, массив предпочтительнее.
3. Когда вставка/удаление в основном происходит в конце: Массивы работают эффективно, когда операции вставки/удаления происходят в конце структуры.

### Когда лучше использовать связный список:
1. Когда важна динамическая структура: Если размер данных заранее неизвестен или часто изменяется, связный список удобнее, так как он динамически меняет свой размер без необходимости выделения или перераспределения памяти.
2. Когда требуется частая вставка или удаление в начале или середине: Связные списки могут эффективно обрабатывать вставку и удаление в начале за `O(1)` (особенно для односвязного списка), тогда как массивы требуют сдвига элементов (`O(n)`).
3. Когда работа с памятью критична: Связные списки могут более эффективно использовать память при большом количестве изменений структуры данных, так как не требуют выделения заранее фиксированного блока памяти.

### Заключение:
* Массив лучше подходит для случаев, когда вам нужен быстрый доступ к элементам по индексу и когда размер данных известен заранее.

* Связный список лучше для сценариев, где часты операции вставки и удаления, особенно в начале или середине списка, а также когда размер данных может динамически изменяться.

[наверх](#big-o)

## Какова сложность алгоритма сортировки кучей (`heapsort`)

### Сложность алгоритма сортировки кучей (`Heapsort`):
* Худший случай: O(n log n)
* Средний случай: O(n log n)
* Лучший случай: O(n log n)

### Объяснение:
* `Heapsort` использует структуру данных кучу (`heap`), которая представляет собой бинарное дерево, удовлетворяющее свойству кучи. Алгоритм состоит из двух основных шагов:

    1. Построение кучи из неотсортированного массива.

    2. Извлечение максимального элемента (в случае макс-кучи) и его перемещение в конец массива, с последующей перестройкой кучи.

### Разбор шагов:
1. Построение кучи:
    * Построение кучи из неотсортированного массива занимает `O(n)`, потому что на каждом уровне дерева требуется все меньше операций для поддержания свойства кучи.

2. Извлечение элементов и перестройка кучи:
    * После того как куча построена, алгоритм многократно извлекает наибольший элемент из кучи (который находится в корне) и перемещает его в конец массива.
    
    * После каждого извлечения выполняется операция восстановления кучи, которая требует `O(log n)` времени (восстановление кучи после удаления корневого элемента).
    * Поскольку нужно выполнить n таких операций, общая сложность этого шага составляет `O(n log n)`.
    
### Итог:
* `Heapsort` работает за `O(n log n)` во всех случаях (худший, средний и лучший), что делает его довольно предсказуемым по времени выполнения.

* `Heapsort` не требует дополнительной памяти, поскольку сортировка выполняется на месте (`in-place`), что отличает его от таких алгоритмов, как сортировка слиянием, которая требует дополнительной памяти.

### Заключение:
Алгоритм Heapsort имеет временную сложность O(n log n) в худшем, среднем и лучшем случаях, и это делает его эффективным и стабильным выбором для сортировки.

[наверх](#big-o)

## Какова сложность алгоритма поиска в графе (например, `DFS` и `BFS`)

### Временная сложность алгоритмов поиска в графе:
1. Поиск в глубину (`DFS` — `Depth-First Search`):

    * Временная сложность: `O(V + E)`
    * Где `V` — это количество вершин (`nodes`), а `E` — количество рёбер (`edges`) в графе.

2. Поиск в ширину (`BFS` — `Breadth-First Search`):
    * Временная сложность: `O(V + E)`
    * Так же как и `DFS`, сложность зависит от количества вершин и рёбер в графе.

### Объяснение:
* Обе операции (`DFS` и `BFS`) требуют пройти через каждую вершину и каждое ребро хотя бы один раз.

* При выполнении `DFS` алгоритм углубляется вдоль каждого пути до его конца, а затем откатывается назад, проверяя другие пути.

* `BFS` исследует каждый уровень графа, начиная с исходной вершины, и проходит по рёбрам на всех соседних уровнях перед переходом на следующий уровень.

* В обоих случаях каждая вершина посещается один раз, а каждое ребро исследуется один раз.

### Итог:
* Временная сложность и для `DFS`, и для `BFS`: `O(V + E)`.
    * `V` (вершины) и `E` (рёбра) — это количество вершин и рёбер в графе соответственно.
    * Оба алгоритма проходят по всем вершинам и рёбрам один раз за время выполнения.

[наверх](#big-o)

## Какова сложность создания бинарного дерева поиска

## Временная сложность создания бинарного дерева поиска (Binary Search Tree, `BST`):
Создание бинарного дерева поиска (`BST`) происходит путем последовательного добавления элементов в дерево, что зависит от порядка вставки и текущей структуры дерева.

### Сложность добавления одного элемента:
* Лучший случай: `O(log n)` — когда дерево сбалансировано.

* Худший случай: `O(n)` — когда дерево не сбалансировано (например, все элементы добавляются последовательно, образуя вытянутое дерево, похожее на список).

### Общая сложность для создания BST:
1. Худший случай: `O(n²)`
    * Если элементы добавляются в неудачном порядке (например, отсортированном или обратном отсортированном), то дерево становится вырожденным (список), и каждая операция вставки требует `O(n)` времени. Таким образом, для n элементов общая сложность будет `O(n²)`.

2. Лучший случай: `O(n log n)`
    * В случае, если элементы добавляются таким образом, что дерево сбалансировано, каждая вставка занимает `O(log n)` времени. Для n элементов это дает общую сложность `O(n log n)`.
### Заключение:
* Худший случай: `O(n²)` — если дерево вырожденное (несбалансированное).

* Лучший случай: `O(n log n)` — если дерево сбалансировано.

[наверх](#big-o)

## Как `асимптотическая сложность` влияет на производительность с увеличением размера входных данных

Асимптотическая сложность (обозначаемая через `Big O`) показывает, как быстро время выполнения алгоритма или использование памяти увеличивается с ростом входных данных. С ростом размера входных данных, алгоритмы с разной сложностью ведут себя по-разному, и их производительность может сильно отличаться.

### Влияние сложности на производительность:
1. `O(1)` — Константная сложность:

    * Время выполнения не зависит от размера входных данных.

    * Пример: доступ к элементу массива по индексу.

    * Производительность не изменяется при увеличении данных.

    * Пример: если требуется 1 мс для обработки 1000 элементов, то 1 мс потребуется и для 1 миллиона элементов.

2. `O(log n)` — Логарифмическая сложность:

    * Время выполнения увеличивается медленно с ростом входных данных, так как каждый шаг уменьшает область поиска.
    
    * Пример: бинарный поиск.
    
    * При увеличении данных в 10 раз, время выполнения увеличится незначительно.

    * Пример: для входных данных в 1000 элементов требуется 10 шагов, а для 1 миллиона — 20 шагов.

3. `O(n)` — Линейная сложность:

    * Время выполнения увеличивается пропорционально размеру входных данных.

    * Пример: простой перебор массива.

    * При увеличении входных данных в 10 раз, время выполнения также увеличится в 10 раз.

    * Пример: если обработка 1000 элементов занимает 1 мс, то обработка 1 миллиона элементов займет 1000 мс.

4. `O(n log n)` — Линейно-логарифмическая сложность:

    * Производительность уменьшается медленнее, чем при линейной сложности, но быстрее, чем при логарифмической.

    * Пример: алгоритмы сортировки, такие как сортировка слиянием или быстрая сортировка.

    * С увеличением входных данных в 10 раз время увеличится примерно в 10 log n раз.

5. `O(n²)` — Квадратичная сложность:

    * Время выполнения увеличивается квадратично с увеличением размера входных данных.

    * Пример: сортировка вставками, сортировка пузырьком.

    * Если входные данные увеличиваются в 10 раз, время выполнения увеличивается в 100 раз.
    
    * Пример: если для обработки 1000 элементов требуется 1 секунда, то для 10 000 элементов может потребоваться около 100 секунд.

6. `O(2ⁿ)` — Экспоненциальная сложность:

    * Время выполнения очень быстро растет с увеличением входных данных, делая такие алгоритмы непрактичными для больших входных данных.

    * Пример: рекурсивные решения задач, таких как задача о Ханойских башнях.

    * Если входные данные увеличиваются на 1 элемент, время выполнения удваивается.
    
    * Пример: если для обработки 20 элементов нужно 1 секунда, то для 21 элемента потребуется 2 секунды, а для 30 элементов — уже почти 17 минут.

### Влияние на производительность:
* Алгоритмы с более низкой сложностью (O(1), O(log n), O(n)) работают эффективно даже с большими объемами данных.

* Алгоритмы с более высокой сложностью (O(n²), O(2ⁿ)) могут работать быстро только с малыми наборами данных, но с ростом данных их производительность падает очень быстро, делая их непрактичными для больших входных данных.

### Пример:
Представьте задачу обработки данных с 1 миллионом элементов:

* O(1): Выполнение займет постоянное время, например, 1 мс.

* O(log n): Выполнение займет около 20 шагов, что может быть очень быстрым, например, несколько миллисекунд.

* O(n): Время выполнения будет пропорционально 1 миллиону, например, 1 секунда.

* O(n log n): Выполнение займет около 20 миллионов операций, что может занять несколько секунд.

* O(n²): Время выполнения станет неприемлемым — около 1 триллиона операций, что может занять часы или дни.

* O(2ⁿ): Время выполнения становится астрономически большим, даже для небольших входных данных.

### Заключение:

Асимптотическая сложность играет ключевую роль в производительности алгоритмов, особенно при увеличении размера входных данных. Чем выше сложность, тем быстрее растет время выполнения, поэтому важно выбирать алгоритмы с более низкой сложностью для работы с большими данными.

[наверх](#big-o)

## Как изменится временная сложность, если использовать рекурсию вместо итерации

Использование рекурсии вместо итерации само по себе не обязательно меняет временную сложность алгоритма, но может влиять на его эффективность и пространственную сложность (использование памяти). Временная сложность зависит от того, как организован алгоритм, а не от того, рекурсивный он или итеративный.

### Когда временная сложность не меняется:
Если рекурсивный алгоритм является эквивалентом итеративного алгоритма, временная сложность может остаться такой же. Примером этого является:

* `Факториал`: как рекурсивный, так и итеративный подход будут иметь временную сложность O(n).

Рекурсивная версия:

```java
int factorial(int n) {
    if (n == 1) return 1;
    return n * factorial(n - 1);
}
```

Итеративная версия:

```java
int factorial(int n) {
    int result = 1;
    for (int i = 2; i <= n; i++) {
        result *= i;
    }
    return result;
}
```

В обоих случаях сложность будет `O(n)`, так как требуется выполнить `n` умножений.

### Когда временная сложность может измениться:

В некоторых случаях, из-за особенностей работы рекурсивного вызова, сложность может измениться. Например, если рекурсивный алгоритм приводит к дублирующимся вычислениям, это может существенно увеличить временную сложность.

`Пример`: рекурсивное вычисление чисел Фибоначчи:

* Простая рекурсивная реализация вычисления чисел Фибоначчи без использования мемоизации:

```java
int fibonacci(int n) {
    if (n <= 1) return n;
    return fibonacci(n - 1) + fibonacci(n - 2);
}
```
Здесь происходит множество повторных вычислений для одних и тех же значений. В результате временная сложность составляет `O(2ⁿ)`, так как каждый вызов функции порождает два новых вызова. Это намного хуже, чем итеративная версия:

Итеративная версия:

```java
int fibonacciIterative(int n) {
    if (n <= 1) return n;
    int a = 0, b = 1;
    for (int i = 2; i <= n; i++) {
        int temp = a + b;
        a = b;
        b = temp;
    }
    return b;
}
```
Итеративный подход имеет сложность `O(n)`.

### Пример с использованием мемоизации:

Если к рекурсивному решению добавить мемоизацию (запоминание промежуточных результатов), временная сложность снова может стать `O(n)`:

```java
int fibonacciMemo(int n, int[] memo) {
    if (n <= 1) return n;
    if (memo[n] != 0) return memo[n];
    memo[n] = fibonacciMemo(n - 1, memo) + fibonacciMemo(n - 2, memo);
    return memo[n];
}
```
Здесь мы избегаем дублирующихся вызовов за счёт сохранения промежуточных результатов, что улучшает временную сложность до `O(n)`.

### Пространственная сложность:
* Рекурсивные алгоритмы обычно требуют дополнительной памяти для поддержания стека вызовов, что может увеличить пространственную сложность до `O(n)`, если глубина рекурсии равна числу элементов. Например, вычисление факториала рекурсивно требует n вызовов стека, тогда как итеративная версия использует `O(1)` дополнительной памяти.

### Заключение:
* Временная сложность рекурсивных и итеративных алгоритмов может быть одинаковой, если дублирующиеся вычисления в рекурсивной версии избегаются (например, с помощью мемоизации).

* Рекурсия может иногда увеличить временную сложность (например, в случае рекурсивного Фибоначчи).

* Пространственная сложность рекурсивного алгоритма обычно выше, так как каждый вызов функции сохраняется в стеке вызовов.

[наверх](#big-o)

## Какова сложность операций вставки и удаления в бинарной куче

Бинарная куча (`Binary Heap`) — это структура данных, часто используемая для реализации очереди с приоритетом. Операции вставки и удаления в бинарной куче имеют следующие временные сложности:

### 1. Операция вставки (Insertion):

* Сложность: `O(log n)`, где `n` — количество элементов в куче.

* Объяснение: При вставке нового элемента в бинарную кучу, элемент добавляется в конец кучи (в последнюю свободную позицию в дереве), чтобы сохранить структуру полного бинарного дерева. Затем выполняется операция "просеивания вверх" (percolate up), где элемент поднимается на более высокие уровни дерева, если он нарушает свойство кучи (например, в мин-куче — если добавленный элемент меньше своего родителя). Эта операция требует не более log n сравнений, так как высота дерева пропорциональна логарифму от числа элементов.

### 2. Операция удаления (Удаление максимума или минимума — DeleteMax/DeleteMin):

* Сложность: `O(log n)`, где `n` — количество элементов в куче.

* Объяснение: В бинарной куче обычно удаляют максимальный элемент (в макс-куче) или минимальный элемент (в мин-куче), который находится в корне дерева. После удаления элемента на его место перемещается последний элемент, чтобы сохранить структуру полного бинарного дерева. Затем выполняется операция "просеивания вниз" (`percolate down`), где этот элемент опускается по дереву, пока не восстановится свойство кучи. Так как высота дерева log n, операция занимает `O(log n)` времени.

### Пример операций:
1. Вставка:
    * Добавляем элемент в конец кучи, затем "поднимаем" его, если нарушено свойство кучи (например, если элемент больше родителя в макс-куче).
2. Удаление:
    * Удаляем элемент с вершины (максимум или минимум), перемещаем последний элемент на вершину и затем "опускаем" его, восстанавливая свойство кучи.

### Итог:

* Операции вставки и удаления требуют не более `O(log n)` операций, так как они связаны с высотой бинарного дерева, а высота полного бинарного дерева пропорциональна логарифму от количества элементов.

[наверх](#big-o)

## Как определить временную сложность рекурсивных функций

### Определение временной сложности рекурсивных функций:

Для определения временной сложности рекурсивных функций часто используется рекуррентное соотношение и методы его решения. Это позволяет оценить, как количество операций зависит от размера входных данных.

### Основные шаги:
1. Запишите рекуррентное соотношение:

    * Определите, как функция вызывает себя рекурсивно и сколько работы выполняется на каждом уровне рекурсии (вне рекурсивных вызовов).

2. Решите рекуррентное соотношение:

    * Используйте методы для решения рекуррентного соотношения, такие как:
        * Подход мастера (Master Theorem).
        * Метод подстановки (substitution method).
        * Метод деревьев рекурсии (recursion tree method).

### Примеры:
1. Рекурсивное вычисление факториала
```java
int factorial(int n) {
    if (n == 1) return 1;
    return n * factorial(n - 1);
}
```

* Рекуррентное соотношение:
    * T(n) = T(n - 1) + O(1)
    * На каждом уровне выполняется одна операция умножения (O(1)), а затем вызывается рекурсивно factorial(n - 1).

* Решение:
    * Это соотношение говорит о том, что для вычисления факториала нужно выполнить n рекурсивных вызовов, каждый из которых выполняет O(1) операций.
    * Следовательно, временная сложность: O(n).

2. Бинарный поиск
```java
int binarySearch(int[] arr, int left, int right, int target) {
    if (left > right) return -1;
    int mid = (left + right) / 2;
    if (arr[mid] == target) return mid;
    if (arr[mid] > target) return binarySearch(arr, left, mid - 1, target);
    return binarySearch(arr, mid + 1, right, target);
}
```
* Рекуррентное соотношение:
    * T(n) = T(n / 2) + O(1)
    * Мы делим массив пополам на каждом уровне, а операция сравнения выполняется за O(1).
* Решение с использованием мастера:
    * Это классическое рекуррентное соотношение для алгоритмов деления пополам. Решение: O(log n).

3. Рекурсивная сортировка слиянием (Merge Sort)
```java
void mergeSort(int[] arr, int left, int right) {
    if (left < right) {
        int mid = (left + right) / 2;
        mergeSort(arr, left, mid);
        mergeSort(arr, mid + 1, right);
        merge(arr, left, mid, right);
    }
}
```

* Рекуррентное соотношение:
    * T(n) = 2T(n / 2) + O(n)
    * Массив делится на две половины, и каждая половина сортируется рекурсивно. Слияние двух отсортированных половин занимает O(n) времени.

* Решение:
    * Это стандартное рекуррентное соотношение для сортировки слиянием, и оно решается с использованием мастер-теоремы.
    * Ответ: O(n log n).

### Методы решения рекуррентных соотношений:
1. **Мастер-теорема**:

**Мастер-теорема** — это инструмент, который позволяет легко решать рекуррентные соотношения вида:

```scss
T(n) = aT(n / b) + O(n^d)
```
Где:

`a` — количество рекурсивных вызовов.
`b` — насколько уменьшается входной размер.
`n^d` — затраты времени вне рекурсивных вызовов.

Для решения смотрим, как соотносятся n^d и a(b^d):

* Если log_b(a) > d, сложность O(n^log_b(a)).
* Если log_b(a) = d, сложность O(n^d log n).
* Если log_b(a) < d, сложность O(n^d).

Пример: для сортировки слиянием:

* a = 2, b = 2, d = 1, log_b(a) = 1, значит сложность O(n log n).

2. **Метод подстановки**:
Вы предполагаете решение и проверяете его с помощью индукции. Это полезно для более сложных рекурсий, где не применима мастера-теорема.

3. **Дерево рекурсии**:
Построение дерева рекурсии помогает визуализировать количество операций на каждом уровне рекурсии и суммировать их для получения полной временной сложности.

### Заключение:
* Для определения временной сложности рекурсивных функций нужно записать рекуррентное соотношение и решить его с помощью мастера-теоремы, метода подстановки или дерева рекурсии.
* В простых случаях (линейная рекурсия) временная сложность обычно O(n).
* В более сложных случаях (например, разбиение данных пополам) сложность часто O(log n) или O(n log n).

[наверх](#big-o)

## Какова сложность алгоритма Дейкстры для поиска кратчайшего пути

### Временная сложность алгоритма Дейкстры для поиска кратчайшего пути:
Алгоритм Дейкстры предназначен для поиска кратчайших путей от одной вершины до всех остальных вершин в графе с неотрицательными весами рёбер.

Временная сложность алгоритма Дейкстры зависит от используемой структуры данных для хранения вершин и рёбер:

1. **Алгоритм Дейкстры с использованием очереди с приоритетом на базе двоичной кучи (binary heap**):
* Вершины: O((V + E) log V)
    * Здесь V — это количество вершин, а E — количество рёбер в графе.
* Объяснение:
    * Для каждой вершины алгоритм выполняет операцию извлечения минимальной вершины из очереди с приоритетом, что занимает O(log V) времени.
    * Для каждого ребра обновляется расстояние до соседней вершины, что также занимает O(log V) времени (операция изменения приоритета в куче).
    * Поскольку мы выполняем эти операции для каждой вершины и каждого ребра, общее время работы составляет O((V + E) log V).

2. **Алгоритм Дейкстры с использованием очереди с приоритетом на базе фибоначчиевой кучи (Fibonacci heap)**:
* Сложность: O(V log V + E)
* Объяснение:
    * В фибоначчиевой куче операция уменьшения ключа (decrease-key) имеет амортизированную сложность O(1), а извлечение минимального элемента имеет сложность O(log V).
    * Таким образом, общая временная сложность для обработки всех вершин будет O(V log V), а для обработки всех рёбер — O(E).
    * Это улучшает производительность по сравнению с бинарной кучей, особенно для графов с большим количеством рёбер.

### Итоговые временные сложности:
* С использованием бинарной кучи: O((V + E) log V).
* С использованием фибоначчиевой кучи: O(V log V + E).

### Когда применять:
* В большинстве случаев бинарная куча более практична и используется чаще, так как она проще в реализации и в среднем достаточно эффективна.

* Фибоначчиева куча может быть более эффективной для графов с большим количеством рёбер, но она сложнее в реализации и используется реже на практике.

[наверх](#big-o)

## Влияет ли удвоение объема входных данных на время выполнения `O(log n)` алгоритма

Удвоение объема входных данных для алгоритма с временной сложностью `O(log n)` влияет на время выполнения, но влияние является незначительным из-за логарифмического характера роста.

### Объяснение:
* В алгоритме с временной сложностью O(log n) время выполнения растет медленно по сравнению с другими сложностями, такими как O(n) или O(n²). Это означает, что увеличение размера входных данных на порядок приводит к увеличению количества операций всего лишь на небольшое значение.

* Логарифмический рост означает, что при увеличении входных данных вдвое, время выполнения увеличится на одну единицу по отношению к логарифму. Например, если входные данные удваиваются с n до 2n, время выполнения увеличивается следующим образом:

    * До удвоения: время выполнения = log n
    * После удвоения: время выполнения = log(2n) = log n + log 2

Поскольку log 2 — это константа (приблизительно 0.3010), разница между log n и log(2n) будет минимальной. Таким образом, удвоение входных данных приводит к незначительному увеличению времени выполнения.

### Пример:
Если для n = 1 000 алгоритм с O(log n) требует примерно log₂(1 000) ≈ 10 операций, то для n = 2 000 потребуется log₂(2 000) ≈ 11 операций. Удвоение объема входных данных увеличило количество операций всего лишь на 1.

### Заключение:
Да, удвоение объема входных данных влияет на время выполнения O(log n) алгоритма, но это увеличение является незначительным, так как логарифмическая сложность увеличивается очень медленно.

[наверх](#big-o)

## Какова сложность получения размера хеш-таблицы

Операция получения размера хеш-таблицы (то есть количества элементов, находящихся в хеш-таблице) имеет O(1) временную сложность.

### Объяснение:
1. Хеш-таблица (например, HashMap в Java или HashTable в других языках) хранит количество элементов в отдельной переменной, которая обновляется при каждой операции вставки или удаления. Поэтому операция получения размера не требует обхода всех элементов или выполнения сложных вычислений.

2. Операция "получения размера" просто возвращает значение, которое хранится в этой переменной, что занимает постоянное время — O(1).

### Итог:
Сложность получения размера хеш-таблицы: O(1) (константное время).

[наверх](#big-o)

## Какова временная сложность разбиения массива (partitioning) в быстрой сортировке

### Временная сложность разбиения массива (partitioning) в быстрой сортировке:
Операция разбиения массива (`partitioning`) — это ключевая часть алгоритма быстрой сортировки (`QuickSort`), где массив делится на две части относительно опорного элемента (`pivot`).

**Временная сложность**:
* O(n), где n — это количество элементов в массиве.

### Объяснение:
1. Во время разбиения массива алгоритм проходит по каждому элементу один раз, сравнивая его с опорным элементом (pivot) и перемещая его либо в левую, либо в правую часть массива.

2. Независимо от расположения опорного элемента или того, как происходит перемещение, каждый элемент массива просматривается ровно один раз.

Таким образом, каждый вызов функции разбиения массива выполняется за O(n) времени.

### Полная сложность быстрой сортировки:
* Средний случай: O(n log n) — Быстрая сортировка делит массив пополам на каждом уровне рекурсии, и на каждом уровне разбиение выполняется за O(n).
* Худший случай: O(n²) — В случае неудачного выбора опорного элемента (например, если массив отсортирован), разбиение будет крайне неэффективным и приведёт к деградации до O(n²).

### Итог:
* Операция разбиения массива в быстрой сортировке имеет O(n) временную сложность для каждого уровня рекурсии.

[наверх](#big-o)

## Как вычислить пространственную сложность алгоритма

`Пространственная сложность` измеряет сколько памяти (или пространства) использует алгоритм в зависимости от размера входных данных. Она учитывает как постоянное использование памяти, так и дополнительную память, которая может потребоваться для хранения данных, временных переменных, рекурсивных вызовов и других структур данных, используемых алгоритмом.

### Шаги для вычисления пространственной сложности:
1. Определите, сколько памяти требуется для входных данных:
    * Пространственная сложность должна учитывать объём памяти, который занимает сам вход (например, массив, список, строка и т.д.).

    * Если вход состоит из n элементов, входные данные могут занимать O(n) памяти.

2. Учтите память, выделяемую под дополнительные переменные:
    * Учтите память, которая выделяется под дополнительные переменные (например, временные переменные, индексы, указатели и т.д.).
    * Например, если ваш алгоритм использует несколько переменных для хранения индексов, каждая из них занимает O(1) (константное) пространство.

3. Оцените память, требуемую для рекурсивных вызовов (если используется рекурсия):
    * Если алгоритм использует рекурсию, каждый рекурсивный вызов занимает место в стеке вызовов.
    
    * Пространственная сложность для рекурсивных алгоритмов часто зависит от глубины рекурсии. Например, если глубина рекурсии пропорциональна размеру входных данных, стек вызовов будет занимать O(n) пространства.

4. Учтите использование вспомогательных структур данных:
    * Определите, какие дополнительные структуры данных используются (например, массивы, списки, хеш-таблицы, очереди, стеки) и сколько памяти они занимают.
    
    * Например, если алгоритм использует дополнительный массив длиной n, это добавит O(n) к пространственной сложности.

5. Суммируйте все компоненты:
    * Пространственная сложность — это сумма памяти, используемой для входных данных, дополнительных переменных и рекурсивных вызовов. Иногда можно игнорировать константные компоненты (например, небольшое количество переменных) и оставить только наиболее значимый элемент.

### Пример 1: Итеративный алгоритм
```java
int sum(int[] arr) {
    int total = 0;
    for (int i = 0; i < arr.length; i++) {
        total += arr[i];
    }
    return total;
}
```
* Входные данные: массив длиной n → O(n).
* Дополнительные переменные: total и i — две переменные, которые занимают O(1).
* Общая пространственная сложность: O(n), так как единственный фактор, определяющий сложность, — это входной массив.

### Пример 2: Рекурсивный алгоритм (Факториал)
```java
int factorial(int n) {
    if (n == 1) return 1;
    return n * factorial(n - 1);
}
```
* Входные данные: одно число n, которое занимает O(1) пространства.
* Рекурсия: стек вызовов глубиной n, так как рекурсия продолжается до тех пор, пока не достигнется базовый случай (n = 1).
* Общая пространственная сложность: O(n) (из-за глубины рекурсии).

### Пример 3: Алгоритм сортировки слиянием (Merge Sort)
```java
void mergeSort(int[] arr, int left, int right) {
    if (left < right) {
        int mid = (left + right) / 2;
        mergeSort(arr, left, mid);
        mergeSort(arr, mid + 1, right);
        merge(arr, left, mid, right);
    }
}
```
* Входные данные: массив длиной n → O(n).
* Дополнительная память: каждый вызов merge() использует временные массивы для слияния двух половин, что требует O(n) пространства на каждом уровне.
* Рекурсия: рекурсия углубляется до log n уровней.
* Общая пространственная сложность: O(n) для временных массивов, используемых для слияния.

### Итог:
* Пространственная сложность алгоритма показывает, сколько памяти требуется для выполнения алгоритма.

* Вычисление включает анализ входных данных, использования дополнительных переменных, рекурсии и вспомогательных структур данных.

* Оценка пространственной сложности обычно выражается в виде Big O, которая описывает рост использования памяти в зависимости от размера входных данных.

[наверх](#big-o)


[назад](../README.md)

